{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bf2402a-b6d6-4a01-9e2a-02933ade96d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models into: /Users/kaushalshivaprakash/Desktop/project3/models\n",
      "[baseline_mean] MAE = 31.1982, saved to /Users/kaushalshivaprakash/Desktop/project3/models/baseline_mean.pkl\n",
      "🏃 View run baseline_mean at: https://dagshub.com/kaushal-shivaprakashan/final_project.mlflow/#/experiments/0/runs/891aaaf99d2b471e9b7ee4fae8f8ae23\n",
      "🧪 View experiment at: https://dagshub.com/kaushal-shivaprakashan/final_project.mlflow/#/experiments/0\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000794 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5326\n",
      "[LightGBM] [Info] Number of data points in the train set: 6582, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 45.662413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/05/10 20:26:20 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lgbm_28lag] MAE = 8.2161, saved to /Users/kaushalshivaprakash/Desktop/project3/models/lgbm_28lag.pkl\n",
      "🏃 View run lgbm_28lag at: https://dagshub.com/kaushal-shivaprakashan/final_project.mlflow/#/experiments/0/runs/98e13fbf89ca4f0c9161741afe70986c\n",
      "🧪 View experiment at: https://dagshub.com/kaushal-shivaprakashan/final_project.mlflow/#/experiments/0\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000929 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5326\n",
      "[LightGBM] [Info] Number of data points in the train set: 6582, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 45.662413\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001089 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1904\n",
      "[LightGBM] [Info] Number of data points in the train set: 6582, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 45.662413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/05/10 20:26:30 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lgbm_top10_imp] MAE = 8.3349, saved to /Users/kaushalshivaprakash/Desktop/project3/models/lgbm_top10_imp.pkl\n",
      "🏃 View run lgbm_top10_imp at: https://dagshub.com/kaushal-shivaprakashan/final_project.mlflow/#/experiments/0/runs/74c909cf936444f79ba1bcc37591cea4\n",
      "🧪 View experiment at: https://dagshub.com/kaushal-shivaprakashan/final_project.mlflow/#/experiments/0\n",
      "✅ Best model 'lgbm_28lag' (MAE=8.2161) saved to /Users/kaushalshivaprakash/Desktop/project3/models/best_model.pkl\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "train_and_save_models.py\n",
    "\n",
    "Loads cleaned Citibike data, then:\n",
    "  1) trains and logs a baseline mean model\n",
    "  2) trains and logs a LightGBM on 28 lag features\n",
    "  3) trains and logs a LightGBM on top-10 importance features\n",
    "\n",
    "All runs go to your DagsHub MLflow server, and each trained model is also saved locally under `models/`.\n",
    "The best-performing model (lowest MAE) is additionally saved as `best_model.pkl`.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import mlflow\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# DagsHub MLflow settings\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"kaushal-shivaprakashan\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"b01d7b8c94b982d47d0224ea469bbfe4b8870ff6\"\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/kaushal-shivaprakashan/final_project.mlflow\")\n",
    "mlflow.set_experiment(\"CitiBike_Remote_Experiment\")\n",
    "\n",
    "# Data & split config\n",
    "PARQUET_PATH = \"/Users/kaushalshivaprakash/Desktop/project3/data/processed/cleaned_citibike/citibike_2023_top3.parquet\"\n",
    "TRAIN_FRAC   = 0.8\n",
    "MAX_LAG      = 28\n",
    "TOP_K        = 10\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Determine models directory (works in scripts and notebooks)\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).parent\n",
    "except NameError:\n",
    "    BASE_DIR = Path.cwd()\n",
    "\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Saving models into:\", MODEL_DIR)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def load_and_agg(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    df[\"datetime\"] = df[\"started_at\"].dt.floor(\"H\")\n",
    "    agg = (\n",
    "        df\n",
    "        .groupby(\"datetime\")\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "        .sort_values(\"datetime\")\n",
    "    )\n",
    "    return agg\n",
    "\n",
    "def train_test_split_ts(df, frac):\n",
    "    idx = int(len(df) * frac)\n",
    "    return df.iloc[:idx], df.iloc[idx:]\n",
    "\n",
    "def log_baseline(train, test):\n",
    "    run_name = \"baseline_mean\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mean_val = train[\"count\"].mean()\n",
    "        preds = [mean_val] * len(test)\n",
    "        mae = mean_absolute_error(test[\"count\"], preds)\n",
    "        mlflow.log_param(\"model_type\", run_name)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "\n",
    "        # Wrap into DummyRegressor so we can save it\n",
    "        dummy = DummyRegressor(strategy=\"constant\", constant=mean_val)\n",
    "        dummy.fit(train[[\"count\"]], train[\"count\"])\n",
    "        out_path = MODEL_DIR / f\"{run_name}.pkl\"\n",
    "        joblib.dump(dummy, out_path)\n",
    "        print(f\"[{run_name}] MAE = {mae:.4f}, saved to {out_path}\")\n",
    "        return dummy, mae\n",
    "\n",
    "def log_lag_model(df):\n",
    "    run_name = \"lgbm_28lag\"\n",
    "    df_lag = df.copy()\n",
    "    for lag in range(1, MAX_LAG + 1):\n",
    "        df_lag[f\"lag_{lag}\"] = df_lag[\"count\"].shift(lag)\n",
    "    df_lag = df_lag.dropna().reset_index(drop=True)\n",
    "\n",
    "    train, test = train_test_split_ts(df_lag, TRAIN_FRAC)\n",
    "    feats = [f\"lag_{i}\" for i in range(1, MAX_LAG + 1)]\n",
    "    X_train, y_train = train[feats], train[\"count\"]\n",
    "    X_test,  y_test  = test[feats],  test[\"count\"]\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.log_param(\"model_type\", run_name)\n",
    "        mlflow.log_param(\"num_lags\", MAX_LAG)\n",
    "        model = LGBMRegressor(random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "        out_path = MODEL_DIR / f\"{run_name}.pkl\"\n",
    "        joblib.dump(model, out_path)\n",
    "        print(f\"[{run_name}] MAE = {mae:.4f}, saved to {out_path}\")\n",
    "        return model, mae\n",
    "\n",
    "def log_topk_model(df):\n",
    "    run_name = \"lgbm_top10_imp\"\n",
    "    df_lag = df.copy()\n",
    "    for lag in range(1, MAX_LAG + 1):\n",
    "        df_lag[f\"lag_{lag}\"] = df_lag[\"count\"].shift(lag)\n",
    "    df_lag = df_lag.dropna().reset_index(drop=True)\n",
    "\n",
    "    train, test = train_test_split_ts(df_lag, TRAIN_FRAC)\n",
    "    feats = [f\"lag_{i}\" for i in range(1, MAX_LAG + 1)]\n",
    "    X_train, y_train = train[feats], train[\"count\"]\n",
    "    X_test,  y_test  = test[feats], test[\"count\"]\n",
    "\n",
    "    # Fit a base LGBM to get importances\n",
    "    base = LGBMRegressor(random_state=42)\n",
    "    base.fit(X_train, y_train)\n",
    "    importances = pd.Series(base.feature_importances_, index=feats)\n",
    "    top_feats = importances.nlargest(TOP_K).index.tolist()\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.log_param(\"model_type\", run_name)\n",
    "        mlflow.log_param(\"num_lags\", MAX_LAG)\n",
    "        mlflow.log_param(\"selected_feats\", top_feats)\n",
    "        model = LGBMRegressor(random_state=42)\n",
    "        model.fit(X_train[top_feats], y_train)\n",
    "        preds = model.predict(X_test[top_feats])\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "        out_path = MODEL_DIR / f\"{run_name}.pkl\"\n",
    "        joblib.dump(model, out_path)\n",
    "        print(f\"[{run_name}] MAE = {mae:.4f}, saved to {out_path}\")\n",
    "        return model, mae\n",
    "\n",
    "def main():\n",
    "    df = load_and_agg(PARQUET_PATH)\n",
    "    train, test = train_test_split_ts(df, TRAIN_FRAC)\n",
    "\n",
    "    baseline_model, mae_baseline = log_baseline(train, test)\n",
    "    lag_model,      mae_lag      = log_lag_model(df)\n",
    "    topk_model,     mae_topk     = log_topk_model(df)\n",
    "\n",
    "    # Choose best by lowest MAE\n",
    "    metrics = {\n",
    "        \"baseline_mean\": mae_baseline,\n",
    "        \"lgbm_28lag\":    mae_lag,\n",
    "        \"lgbm_top10_imp\": mae_topk,\n",
    "    }\n",
    "    models = {\n",
    "        \"baseline_mean\": baseline_model,\n",
    "        \"lgbm_28lag\":    lag_model,\n",
    "        \"lgbm_top10_imp\": topk_model,\n",
    "    }\n",
    "    best_name = min(metrics, key=metrics.get)\n",
    "    best_model = models[best_name]\n",
    "    best_path  = MODEL_DIR / \"best_model.pkl\"\n",
    "    joblib.dump(best_model, best_path)\n",
    "    print(f\"✅ Best model '{best_name}' (MAE={metrics[best_name]:.4f}) saved to {best_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cfd563-b3ca-4448-80bc-53c468b2c06b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
