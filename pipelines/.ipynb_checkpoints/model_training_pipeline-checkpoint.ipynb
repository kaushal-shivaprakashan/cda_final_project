{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e55e787-5225-4add-b6fc-74d213f9e0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 75\n",
      "[LightGBM] [Info] Number of data points in the train set: 17593, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 16.700335\n",
      "Validation MAE: 5.541\n",
      "âœ” Saved feature pipeline -> /Users/kaushalshivaprakash/Desktop/project3/pipelines/models/feature_pipeline.pkl\n",
      "âœ” Saved model            -> /Users/kaushalshivaprakash/Desktop/project3/pipelines/models/model_training_pipeline.pkl\n",
      "ðŸš€ Model training pipeline executed successfully and saved as 'model_training_pipeline.pkl'!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kaushal/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# training_pipeline.py\n",
    "\"\"\"\n",
    "Standalone model training pipeline.\n",
    "Loads raw data, aggregates rides into hourly counts,\n",
    "trains a LightGBM model with preprocessing pipeline,\n",
    "saves artifacts to disk, and prints confirmation.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMRegressor\n",
    "from joblib import dump\n",
    "\n",
    "# â”€â”€ 1) CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PARQUET_PATH = \"/Users/kaushalshivaprakash/Desktop/project3/data/processed/cleaned_citibike/citibike_2023_top3.parquet\"\n",
    "MODEL_DIR    = \"/Users/kaushalshivaprakash/Desktop/project3/pipelines/models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# â”€â”€ 2) DATA LOADING & AGGREGATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Read ride-level Parquet data\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "df[\"started_at\"] = pd.to_datetime(df[\"started_at\"])\n",
    "# Bucket into hourly periods\n",
    "df[\"hour_bucket\"] = df[\"started_at\"].dt.floor(\"H\")\n",
    "# Aggregate trips per station per hour\n",
    "agg = (\n",
    "    df.groupby([\"start_station_id\", \"hour_bucket\"])  \n",
    "      .agg(target_trips=(\"ride_id\", \"count\"))\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# â”€â”€ 3) Prepare features and target â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X = agg[[\"start_station_id\", \"hour_bucket\"]]\n",
    "y = agg[\"target_trips\"]\n",
    "\n",
    "# â”€â”€ 4) Define preprocessing pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def select_hour_bucket(df):\n",
    "    return df[[\"hour_bucket\"]]\n",
    "\n",
    "def extract_datetime_parts(df):\n",
    "    return pd.DataFrame({\n",
    "        \"year\":  df[\"hour_bucket\"].dt.year,\n",
    "        \"month\": df[\"hour_bucket\"].dt.month,\n",
    "        \"day\":   df[\"hour_bucket\"].dt.day,\n",
    "        \"hour\":  df[\"hour_bucket\"].dt.hour\n",
    "    })\n",
    "\n",
    "def select_station_id(df):\n",
    "    return df[[\"start_station_id\"]]\n",
    "\n",
    "datetime_pipeline = Pipeline([\n",
    "    (\"select_dt\", FunctionTransformer(select_hour_bucket, validate=False)),\n",
    "    (\"extract\",   FunctionTransformer(extract_datetime_parts, validate=False))\n",
    "])\n",
    "\n",
    "station_pipeline = Pipeline([\n",
    "    (\"select_loc\", FunctionTransformer(select_station_id, validate=False)),\n",
    "    (\"onehot\",     OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"dt_feats\",  datetime_pipeline,    [\"hour_bucket\"]),\n",
    "    (\"loc_feats\", station_pipeline,     [\"start_station_id\"]),\n",
    "])\n",
    "\n",
    "# â”€â”€ 5) Create full modeling pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimator\",    LGBMRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# â”€â”€ 6) Train/test split & fit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# â”€â”€ 7) Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "val_preds = pipeline.predict(X_val)\n",
    "mae = abs(val_preds - y_val).mean()\n",
    "print(f\"Validation MAE: {mae:.3f}\")\n",
    "\n",
    "# â”€â”€ 8) Save artifacts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "feature_pipeline_path = os.path.join(MODEL_DIR, \"feature_pipeline.pkl\")\n",
    "# Save model under the name 'model_training_pipeline.pkl'\n",
    "model_path            = os.path.join(MODEL_DIR, \"model_training_pipeline.pkl\")\n",
    "\n",
    "dump(preprocessor, feature_pipeline_path)\n",
    "print(f\"âœ” Saved feature pipeline -> {feature_pipeline_path}\")\n",
    "\n",
    "dump(pipeline.named_steps[\"estimator\"], model_path)\n",
    "print(f\"âœ” Saved model            -> {model_path}\")\n",
    "\n",
    "# â”€â”€ 9) Final confirmation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸš€ Model training pipeline executed successfully and saved as 'model_training_pipeline.pkl'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0936793f-4b49-4573-86fc-c2bf01c27d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
