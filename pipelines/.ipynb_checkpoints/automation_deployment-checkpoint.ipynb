{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98cd8865-1186-4db3-a0e4-eb7b32eeb3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 17:11:26,108 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-05-10 17:11:26,112 INFO: Initializing external client\n",
      "2025-05-10 17:11:26,112 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-05-10 17:11:26,829 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1213683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |█| Rows 8256/8256 | Elapsed Time: 00:00 | Remainin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: citi_bike_hourly_counts_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1213683/jobs/named/citi_bike_hourly_counts_1_offline_fg_materialization/executions\n",
      "→ Inserted 8256 rows into Feature Group 'citi_bike_hourly_counts'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |█| Rows 8228/8228 | Elapsed Time: 00:01 | Remainin\n",
      "UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1213683/jobs/named/citi_bike_hourly_predictions_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/Applied_ML_Project1/Resources/jobs/citi_bike_hourly_predictions_1_offline_fg_materialization/config_1746911103706) to trigger the materialization job again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Inserted 8228 rows into Feature Group 'citi_bike_hourly_predictions'\n",
      "→ Best run 305b68c9cd9c463fb359e5b715a94872 with MAE=8.22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb47698cebf7405fb074f6b0234d7ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Downloaded artifacts to ./best_model_artifact/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProvenanceWarning: Model schema cannot not be inferred without both the feature view and the training dataset version.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738f518f60a045808d759d8764be27a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18cc540705a4e9982b2cec1da9f29eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /Users/kaushalshivaprakash/Desktop/project3/pipelines/best_model_artifact/model/python_env.yaml: 0.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d62d10efe9a455a9d0ea7440a0d430c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /Users/kaushalshivaprakash/Desktop/project3/pipelines/best_model_artifact/model/requirements.txt: 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15d13fef4374a999aca005b9c1087d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /Users/kaushalshivaprakash/Desktop/project3/pipelines/best_model_artifact/model/MLmodel: 0.000%|    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbed8e6f85f248aea4f883d3c8f4d16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /Users/kaushalshivaprakash/Desktop/project3/pipelines/best_model_artifact/model/model.pkl: 0.000%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d25d8e5c7f4de180dd2493a568f346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /Users/kaushalshivaprakash/Desktop/project3/pipelines/best_model_artifact/model/conda.yaml: 0.000%| …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://c.app.hopsworks.ai:443/p/1213683/models/CitiBikeForecasting/2\n",
      "→ Registered model 'CitiBikeForecasting' version 2 in Hopsworks\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ingest_to_hopsworks.py\n",
    "\n",
    "Loads hourly counts (always), optional predictions (if present),\n",
    "and the best MLflow model, then pushes them into Hopsworks\n",
    "Feature Store and Model Registry.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import hopsworks\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# LOCAL PATHS & CONFIG\n",
    "PARQUET_PATH       = \"/Users/kaushalshivaprakash/Desktop/project3/data/processed/cleaned_citibike/citibike_2023_top3.parquet\"\n",
    "PREDICTIONS_PATH   = \"/Users/kaushalshivaprakash/Desktop/project3/data/predictions.csv\"\n",
    "EXPERIMENT_NAME    = \"CitiBike_Remote_Experiment\"\n",
    "\n",
    "# Feature Group names & versions\n",
    "FG_COUNTS_NAME     = \"citi_bike_hourly_counts\"\n",
    "FG_COUNTS_VERSION  = 1\n",
    "FG_PRED_NAME       = \"citi_bike_hourly_predictions\"\n",
    "FG_PRED_VERSION    = 1\n",
    "\n",
    "# Hopsworks Model Registry name\n",
    "HOPS_MODEL_NAME    = \"CitiBikeForecasting\"\n",
    "\n",
    "# Local folder for downloading the best model artifact\n",
    "MODEL_LOCAL_PATH   = \"best_model_artifact\"\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def main():\n",
    "    # 1. Log into Hopsworks\n",
    "    project = hopsworks.login()\n",
    "    fs      = project.get_feature_store()\n",
    "    mr      = project.get_model_registry()\n",
    "\n",
    "    # 2. Load & aggregate your time-series counts\n",
    "    df = pd.read_parquet(PARQUET_PATH)\n",
    "    df[\"datetime\"] = df[\"started_at\"].dt.floor(\"H\")\n",
    "    counts = (\n",
    "        df.groupby(\"datetime\")\n",
    "          .size()\n",
    "          .reset_index(name=\"count\")\n",
    "          .sort_values(\"datetime\")\n",
    "    )\n",
    "\n",
    "    # 3. Create or get a Feature Group for counts\n",
    "    fg_counts = fs.get_feature_group(FG_COUNTS_NAME, FG_COUNTS_VERSION)\n",
    "    if fg_counts is None:\n",
    "        fg_counts = fs.create_feature_group(\n",
    "            name=FG_COUNTS_NAME,\n",
    "            version=FG_COUNTS_VERSION,\n",
    "            primary_key=[\"datetime\"],\n",
    "            event_time=\"datetime\",\n",
    "            description=\"Hourly ride counts for top-3 stations\",\n",
    "        )\n",
    "    fg_counts.insert(counts, write_options={\"wait_for_job\": False})\n",
    "    print(f\"→ Inserted {len(counts)} rows into Feature Group '{FG_COUNTS_NAME}'\")\n",
    "\n",
    "    # 4. (Optional) Load & ingest your predictions CSV if it exists\n",
    "    if os.path.exists(PREDICTIONS_PATH):\n",
    "        preds = pd.read_csv(PREDICTIONS_PATH, parse_dates=[\"datetime\"])\n",
    "        fg_preds = fs.get_feature_group(FG_PRED_NAME, FG_PRED_VERSION)\n",
    "        if fg_preds is None:\n",
    "            fg_preds = fs.create_feature_group(\n",
    "                name=FG_PRED_NAME,\n",
    "                version=FG_PRED_VERSION,\n",
    "                primary_key=[\"datetime\"],\n",
    "                event_time=\"datetime\",\n",
    "                description=\"Model predictions for hourly ride counts\",\n",
    "            )\n",
    "        fg_preds.insert(preds, write_options={\"wait_for_job\": False})\n",
    "        print(f\"→ Inserted {len(preds)} rows into Feature Group '{FG_PRED_NAME}'\")\n",
    "    else:\n",
    "        print(f\"⚠️  No predictions file at '{PREDICTIONS_PATH}', skipped.\")\n",
    "\n",
    "    # 5. Find the best MLflow run (lowest MAE) in your experiment\n",
    "    client = MlflowClient()\n",
    "    exp    = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    runs   = client.search_runs(\n",
    "        experiment_ids=[exp.experiment_id],\n",
    "        run_view_type=1,\n",
    "        max_results=1,\n",
    "        order_by=[\"metrics.mae ASC\"],\n",
    "    )\n",
    "    best   = runs[0]\n",
    "    run_id = best.info.run_id\n",
    "    best_mae = best.data.metrics[\"mae\"]\n",
    "    print(f\"→ Best run {run_id} with MAE={best_mae:.2f}\")\n",
    "\n",
    "    # 6. Download the model artifact from that run\n",
    "    if os.path.exists(MODEL_LOCAL_PATH):\n",
    "        shutil.rmtree(MODEL_LOCAL_PATH)\n",
    "    os.makedirs(MODEL_LOCAL_PATH, exist_ok=True)\n",
    "    client.download_artifacts(run_id, \"model\", dst_path=MODEL_LOCAL_PATH)\n",
    "    print(f\"→ Downloaded artifacts to ./{MODEL_LOCAL_PATH}/\")\n",
    "\n",
    "    # 7. Register in Hopsworks Model Registry\n",
    "    hs_model = mr.python.create_model(HOPS_MODEL_NAME)\n",
    "    hs_model.save(MODEL_LOCAL_PATH)\n",
    "    print(f\"→ Registered model '{HOPS_MODEL_NAME}' version {hs_model.version} in Hopsworks\")\n",
    "\n",
    "    # If you wish to promote to production, you can do so via:\n",
    "    #   - The Hopsworks UI under Model Registry\n",
    "    #   - or via the Hopsworks SDK, e.g. mr.get_model(...).set_stage(\"production\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc705f5a-50e7-411d-b6fa-f2e139f0c649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run: 305b68c9cd9c463fb359e5b715a94872 (MAE=8.22)\n",
      "Loading model from runs:/305b68c9cd9c463fb359e5b715a94872/model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d517aa630d487ba57e283c081b852f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 8228 rows to /Users/kaushalshivaprakash/Desktop/project3/data/predictions.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "inference_run_uri.py\n",
    "\n",
    "Finds your best MLflow run (by MAE), loads its model artifact directly via run URI,\n",
    "makes forecasts on the cleaned Parquet, and writes out predictions.csv.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# CONFIG\n",
    "PARQUET_PATH    = \"/Users/kaushalshivaprakash/Desktop/project3/data/processed/cleaned_citibike/citibike_2023_top3.parquet\"\n",
    "EXPERIMENT_NAME = \"CitiBike_Remote_Experiment\"\n",
    "OUTPUT_CSV      = \"/Users/kaushalshivaprakash/Desktop/project3/data/predictions.csv\"\n",
    "\n",
    "# If you’re using DagsHub remote tracking, set these (otherwise unset)\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"kaushal-shivaprakashan\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"b01d7b8c94b982d47d0224ea469bbfe4b8870ff6\"\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/kaushal-shivaprakashan/final_project.mlflow\")\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def load_hourly_counts(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    df[\"datetime\"] = df[\"started_at\"].dt.floor(\"H\")\n",
    "    agg = (\n",
    "        df.groupby(\"datetime\")\n",
    "          .size()\n",
    "          .reset_index(name=\"count\")\n",
    "          .sort_values(\"datetime\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    return agg\n",
    "\n",
    "def get_best_run_id(experiment_name: str) -> str:\n",
    "    client = MlflowClient()\n",
    "    exp = client.get_experiment_by_name(experiment_name)\n",
    "    if exp is None:\n",
    "        raise ValueError(f\"Experiment '{experiment_name}' not found\")\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[exp.experiment_id],\n",
    "        filter_string=\"\",\n",
    "        run_view_type=1,\n",
    "        max_results=1,\n",
    "        order_by=[\"metrics.mae ASC\"]\n",
    "    )\n",
    "    best = runs[0]\n",
    "    print(f\"Best run: {best.info.run_id} (MAE={best.data.metrics['mae']:.2f})\")\n",
    "    return best.info.run_id\n",
    "\n",
    "def load_model_from_run(run_id: str):\n",
    "    uri = f\"runs:/{run_id}/model\"\n",
    "    print(f\"Loading model from {uri}\")\n",
    "    return mlflow.pyfunc.load_model(uri)\n",
    "\n",
    "def build_lag_features(df, max_lag=28):\n",
    "    df = df.copy()\n",
    "    for lag in range(1, max_lag+1):\n",
    "        df[f\"lag_{lag}\"] = df[\"count\"].shift(lag)\n",
    "    return df.dropna().reset_index(drop=True)\n",
    "\n",
    "def main():\n",
    "    # 1) load counts\n",
    "    hourly = load_hourly_counts(PARQUET_PATH)\n",
    "\n",
    "    # 2) find best run\n",
    "    run_id = get_best_run_id(EXPERIMENT_NAME)\n",
    "\n",
    "    # 3) load model\n",
    "    model = load_model_from_run(run_id)\n",
    "\n",
    "    # 4) build features & predict\n",
    "    data = build_lag_features(hourly, max_lag=28)\n",
    "    feature_cols = [f\"lag_{i}\" for i in range(1, 29)]\n",
    "    data[\"prediction\"] = model.predict(data[feature_cols])\n",
    "\n",
    "    # 5) write CSV\n",
    "    out = data[[\"datetime\", \"prediction\"]]\n",
    "    os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "    out.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"Wrote {len(out)} rows to {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b5b12e-cfbf-4452-ab34-d18c53984f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "693b72d1-7609-44aa-a3ae-9b564ad1c42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 00:35:28,444 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-05-11 00:35:28,448 INFO: Initializing external client\n",
      "2025-05-11 00:35:28,448 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-05-11 00:35:29,043 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1228970\n",
      "✅ Logged into Hopsworks: final_project_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProvenanceWarning: Model schema cannot not be inferred without both the feature view and the training dataset version.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa470b649917435ebd8a4330020a6350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7bb194cc1d40ac944c780ae5b421bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /tmp/hopsworks_model_export/model.pkl: 0.000%|          | 0/276812 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://c.app.hopsworks.ai:443/p/1228970/models/citi_bike_demand_model/6\n",
      "✅ Registered model version: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |█| Rows 26294/26294 | Elapsed Time: 00:00 | Remain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: citi_bike_inference_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1228970/jobs/named/citi_bike_inference_1_offline_fg_materialization/executions\n",
      "2025-05-11 00:35:51,106 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2025-05-11 00:35:54,198 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2025-05-11 00:37:30,185 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2025-05-11 00:37:30,251 INFO: Waiting for log aggregation to finish.\n",
      "2025-05-11 00:37:41,688 INFO: Execution finished successfully.\n",
      "✅ Stored inference results in Feature Store.\n"
     ]
    },
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/1228970/serving). Server response: \nHTTP code: 422, HTTP reason: CUSTOM, body: b'{\"errorCode\":120001,\"usrMsg\":\"Serving name must consist of lower case alphanumeric characters, \\'-\\' or \\'.\\', and start and end with an alphanumeric character\",\"errorMsg\":\"An argument was not provided or it was malformed.\"}', error code: 120001, error msg: An argument was not provided or it was malformed., user msg: Serving name must consist of lower case alphanumeric characters, '-' or '.', and start and end with an alphanumeric character",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRestAPIError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Stored inference results in Feature Store.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# ─── 6) DEPLOY MODEL ENDPOINT ────────────────────────────────────────────\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m endpoint = \u001b[43msk_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEP_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🚀 Deployed endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/kaushal/lib/python3.11/site-packages/hopsworks_common/usage.py:246\u001b[39m, in \u001b[36mmethod_logger.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    245\u001b[39m     exception = e\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/kaushal/lib/python3.11/site-packages/hopsworks_common/usage.py:242\u001b[39m, in \u001b[36mmethod_logger.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    239\u001b[39m exception = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# Call the original method\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/kaushal/lib/python3.11/site-packages/hsml/model.py:270\u001b[39m, in \u001b[36mModel.deploy\u001b[39m\u001b[34m(self, name, description, artifact_version, serving_tool, script_file, config_file, resources, inference_logger, inference_batcher, transformer, api_protocol, environment)\u001b[39m\n\u001b[32m    252\u001b[39m     name = \u001b[38;5;28mself\u001b[39m._get_default_serving_name()\n\u001b[32m    254\u001b[39m predictor = Predictor.for_model(\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    256\u001b[39m     name=name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m     environment=environment,\n\u001b[32m    268\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/kaushal/lib/python3.11/site-packages/hsml/predictor.py:140\u001b[39m, in \u001b[36mPredictor.deploy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create a deployment for this predictor and persists it in the Model Serving.\u001b[39;00m\n\u001b[32m    110\u001b[39m \n\u001b[32m    111\u001b[39m \u001b[33;03m!!! example\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    134\u001b[39m \u001b[33;03m    `Deployment`. The deployment metadata object of a new or existing deployment.\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    137\u001b[39m _deployment = deployment.Deployment(\n\u001b[32m    138\u001b[39m     predictor=\u001b[38;5;28mself\u001b[39m, name=\u001b[38;5;28mself\u001b[39m._name, description=\u001b[38;5;28mself\u001b[39m._description\n\u001b[32m    139\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m \u001b[43m_deployment\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _deployment\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/kaushal/lib/python3.11/site-packages/hopsworks_common/usage.py:246\u001b[39m, in \u001b[36mmethod_logger.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    245\u001b[39m     exception = e\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/kaushal/lib/python3.11/site-packages/hopsworks_common/usage.py:242\u001b[39m, in \u001b[36mmethod_logger.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    239\u001b[39m exception = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# Call the original method\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/kaushal/lib/python3.11/site-packages/hsml/deployment.py:81\u001b[39m, in \u001b[36mDeployment.save\u001b[39m\u001b[34m(self, await_update)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;129m@usage\u001b[39m.method_logger\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, await_update: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[32m600\u001b[39m):\n\u001b[32m     71\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Persist this deployment including the predictor and metadata to Model Serving.\u001b[39;00m\n\u001b[32m     72\u001b[39m \n\u001b[32m     73\u001b[39m \u001b[33;03m    # Arguments\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m \u001b[33;03m        `hopsworks.client.exceptions.RestAPIError`: In case the backend encounters an issue\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serving_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mawait_update\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/kaushal/lib/python3.11/site-packages/hsml/engine/serving_engine.py:495\u001b[39m, in \u001b[36mServingEngine.save\u001b[39m\u001b[34m(self, deployment_instance, await_update)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, deployment_instance, await_update: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m deployment_instance.id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m         \u001b[38;5;66;03m# if new deployment\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeployment_instance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    498\u001b[39m     \u001b[38;5;66;03m# if existing deployment\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/kaushal/lib/python3.11/site-packages/hsml/engine/serving_engine.py:438\u001b[39m, in \u001b[36mServingEngine.create\u001b[39m\u001b[34m(self, deployment_instance)\u001b[39m\n\u001b[32m    435\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPlease, choose a different name.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    437\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raise_err:\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m re\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deployment_instance.is_stopped():\n\u001b[32m    441\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBefore making predictions, start the deployment by using `.start()`\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/kaushal/lib/python3.11/site-packages/hsml/engine/serving_engine.py:415\u001b[39m, in \u001b[36mServingEngine.create\u001b[39m\u001b[34m(self, deployment_instance)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\u001b[38;5;28mself\u001b[39m, deployment_instance):\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serving_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeployment_instance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDeployment created, explore it at \u001b[39m\u001b[33m\"\u001b[39m + deployment_instance.get_url())\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m RestAPIError \u001b[38;5;28;01mas\u001b[39;00m re:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/kaushal/lib/python3.11/site-packages/hsml/core/serving_api.py:137\u001b[39m, in \u001b[36mServingApi.put\u001b[39m\u001b[34m(self, deployment_instance)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deployment_instance.artifact_version == ARTIFACT_VERSION.CREATE:\n\u001b[32m    134\u001b[39m     deployment_instance.artifact_version = -\u001b[32m1\u001b[39m\n\u001b[32m    136\u001b[39m deployment_instance = deployment_instance.update_from_response_json(\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPUT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeployment_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m )\n\u001b[32m    144\u001b[39m deployment_instance.model_registry_id = _client._project_id\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m deployment_instance\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/kaushal/lib/python3.11/site-packages/hopsworks_common/decorators.py:48\u001b[39m, in \u001b[36mconnected.<locals>.if_connected\u001b[39m\u001b[34m(inst, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst._connected:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/kaushal/lib/python3.11/site-packages/hopsworks_common/client/base.py:186\u001b[39m, in \u001b[36mClient._send_request\u001b[39m\u001b[34m(self, method, path_params, query_params, headers, data, stream, files, with_base_path_params)\u001b[39m\n\u001b[32m    181\u001b[39m     response = \u001b[38;5;28mself\u001b[39m._retry_token_expired(\n\u001b[32m    182\u001b[39m         request, stream, \u001b[38;5;28mself\u001b[39m.TOKEN_EXPIRED_RETRY_INTERVAL, \u001b[32m1\u001b[39m\n\u001b[32m    183\u001b[39m     )\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code // \u001b[32m100\u001b[39m != \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.RestAPIError(url, response)\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[31mRestAPIError\u001b[39m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/1228970/serving). Server response: \nHTTP code: 422, HTTP reason: CUSTOM, body: b'{\"errorCode\":120001,\"usrMsg\":\"Serving name must consist of lower case alphanumeric characters, \\'-\\' or \\'.\\', and start and end with an alphanumeric character\",\"errorMsg\":\"An argument was not provided or it was malformed.\"}', error code: 120001, error msg: An argument was not provided or it was malformed., user msg: Serving name must consist of lower case alphanumeric characters, '-' or '.', and start and end with an alphanumeric character"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "register_and_deploy_model.py\n",
    "\n",
    "1) Defines unpickle‐time helpers\n",
    "2) Logs into Hopsworks\n",
    "3) Loads preprocessing pipeline & 28-lag model\n",
    "4) Registers model via the sklearn API (exporting only the .pkl)\n",
    "5) Generates 28-lag features, runs batch inference, writes predictions\n",
    "6) Deploys the model endpoint (single .pkl artifact)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import hopsworks\n",
    "\n",
    "# ─── 1) UNPICKLE HELPERS ────────────────────────────────────────────────\n",
    "def select_hour_bucket(df): return df[[\"hour_bucket\"]]\n",
    "def extract_datetime_parts(df):\n",
    "    return pd.DataFrame({\n",
    "        \"year\":  df[\"hour_bucket\"].dt.year,\n",
    "        \"month\": df[\"hour_bucket\"].dt.month,\n",
    "        \"day\":   df[\"hour_bucket\"].dt.day,\n",
    "        \"hour\":  df[\"hour_bucket\"].dt.hour\n",
    "    })\n",
    "def select_station_id(df): return df[[\"start_station_id\"]]\n",
    "\n",
    "# 28-lag generator\n",
    "def make_28_lag_features(df):\n",
    "    counts = df.groupby([\"start_station_id\",\"hour_bucket\"]).size().rename(\"trips\").reset_index()\n",
    "    frames = []\n",
    "    for station, grp in counts.groupby(\"start_station_id\"):\n",
    "        ts = grp.set_index(\"hour_bucket\")[\"trips\"]\n",
    "        full_idx = pd.date_range(ts.index.min(), ts.index.max(), freq=\"H\")\n",
    "        ts = ts.reindex(full_idx, fill_value=0)\n",
    "        ts.index.name = \"hour_bucket\"\n",
    "        lags = {f\"lag_{i}\": ts.shift(i) for i in range(1,29)}\n",
    "        lag_df = pd.DataFrame(lags).dropna()\n",
    "        lag_df[\"start_station_id\"] = station\n",
    "        frames.append(lag_df.reset_index())\n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# ─── 2) CONFIGURATION ───────────────────────────────────────────────────\n",
    "HOST    = \"c.app.hopsworks.ai\"\n",
    "PROJECT = \"final_project_7\"\n",
    "API_KEY = \"QfxZ4kUDSLb9E9qs.xCFq8U27Q5jfdYDNVLMuwmj5e90Ru3cUC1oXpbzl5WNCXZIJ3SJ1DGM3uaiVHYUV\"\n",
    "\n",
    "PIPELINE_PATH = \"/Users/kaushalshivaprakash/Desktop/project3/pipelines/models/feature_pipeline.pkl\"\n",
    "MODEL_PATH    = \"/Users/kaushalshivaprakash/Desktop/project3/models/lgbm_28lag.pkl\"\n",
    "PARQUET_PATH  = \"/Users/kaushalshivaprakash/Desktop/project3/data/processed/cleaned_citibike/citibike_2023_top3.parquet\"\n",
    "\n",
    "FG_NAME    = \"citi_bike_inference\"\n",
    "FG_VERSION = 1\n",
    "EP_NAME    = \"citiBikePredictor\"  # alphanumeric only\n",
    "\n",
    "# ─── 3) LOGIN & LOAD ARTIFACTS ────────────────────────────────────────────\n",
    "project = hopsworks.login(host=HOST, project=PROJECT, api_key_value=API_KEY)\n",
    "print(f\"✅ Logged into Hopsworks: {project.name}\")\n",
    "\n",
    "preprocessor = joblib.load(PIPELINE_PATH)\n",
    "model        = joblib.load(MODEL_PATH)\n",
    "\n",
    "# ─── 4) REGISTER MODEL ───────────────────────────────────────────────────\n",
    "mr = project.get_model_registry()\n",
    "sk_model = mr.sklearn.create_model(\n",
    "    name=\"citi_bike_demand_model\",\n",
    "    metrics={\"validation_mae\": 5.534},\n",
    "    description=\"LGBM 28-lag model\"\n",
    ")\n",
    "export_dir = \"/tmp/hopsworks_model_export\"\n",
    "shutil.rmtree(export_dir, ignore_errors=True)\n",
    "os.makedirs(export_dir)\n",
    "# Only export the single model file for serving:\n",
    "shutil.copy(MODEL_PATH, os.path.join(export_dir, \"model.pkl\"))\n",
    "sk_model.save(export_dir)\n",
    "print(f\"✅ Registered model version: {sk_model.version}\")\n",
    "\n",
    "# ─── 5) BATCH INFERENCE & STORE ─────────────────────────────────────────\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "df[\"started_at\"]  = pd.to_datetime(df[\"started_at\"])\n",
    "df[\"hour_bucket\"] = df[\"started_at\"].dt.floor(\"H\")\n",
    "\n",
    "lags_df = make_28_lag_features(df)\n",
    "lag_cols = [f\"lag_{i}\" for i in range(1,29)]\n",
    "lags_df[\"predicted_trips\"] = model.predict(lags_df[lag_cols])\n",
    "\n",
    "fs = project.get_feature_store()\n",
    "fg = fs.get_or_create_feature_group(\n",
    "    name=FG_NAME,\n",
    "    version=FG_VERSION,\n",
    "    primary_key=[\"start_station_id\",\"hour_bucket\"],\n",
    "    event_time=\"hour_bucket\",\n",
    "    description=\"28-lag batch predictions\"\n",
    ")\n",
    "fg.insert(lags_df[[\"start_station_id\",\"hour_bucket\",\"predicted_trips\"]],\n",
    "          write_options={\"wait_for_job\":True})\n",
    "print(\"✅ Stored inference results in Feature Store.\")\n",
    "\n",
    "# ─── 6) DEPLOY MODEL ENDPOINT ────────────────────────────────────────────\n",
    "endpoint = sk_model.deploy(name=EP_NAME)\n",
    "print(f\"🚀 Deployed endpoint: {endpoint.name} at {endpoint.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8d982-de4b-4af1-9da4-ea5154603ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
