{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c21b93-eadb-45c7-820b-ec92697c71da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline built. Train samples: 20161, Test samples: 1663\n",
      "Feature engineering pipeline successfully saved to models/feature_pipeline.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kaushal/lib/python3.11/site-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# feature_engineering_pipeline.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "\n",
    "# --- Feature Generators ---\n",
    "class FilterTopLocations(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Keeps only records for specified top-N locations.\n",
    "    \"\"\"\n",
    "    def __init__(self, top_locations):\n",
    "        self.top_locations = top_locations\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[X['location_id'].isin(self.top_locations)].copy()\n",
    "\n",
    "class DateTimeFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Extracts hour, day of week, and month from a datetime column.\n",
    "    \"\"\"\n",
    "    def __init__(self, datetime_column='datetime'):\n",
    "        self.datetime_column = datetime_column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        dt = pd.to_datetime(X_[self.datetime_column])\n",
    "        X_['hour'] = dt.dt.hour\n",
    "        X_['dayofweek'] = dt.dt.dayofweek\n",
    "        X_['month'] = dt.dt.month\n",
    "        return X_\n",
    "\n",
    "class LagFeatureGenerator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Generates lag features for each location-level time series.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_lags=28, group_column='location_id', datetime_column='datetime', target_column='trip_count'):\n",
    "        self.n_lags = n_lags\n",
    "        self.group_column = group_column\n",
    "        self.datetime_column = datetime_column\n",
    "        self.target_column = target_column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        X_.sort_values(by=[self.group_column, self.datetime_column], inplace=True)\n",
    "        for lag in range(1, self.n_lags + 1):\n",
    "            X_[f'lag_{lag}'] = X_.groupby(self.group_column)[self.target_column].shift(lag)\n",
    "        return X_.dropna().reset_index(drop=True)\n",
    "\n",
    "# --- Pipeline Builder ---\n",
    "def build_feature_pipeline(top_locations, n_lags=28):\n",
    "    feature_steps = [\n",
    "        ('filter', FilterTopLocations(top_locations)),\n",
    "        ('dt', DateTimeFeatures('datetime')),\n",
    "        ('lag', LagFeatureGenerator(n_lags, 'location_id', 'datetime', 'trip_count'))\n",
    "    ]\n",
    "    feature_pipeline = Pipeline(feature_steps)\n",
    "\n",
    "    datetime_feats = ['hour', 'dayofweek', 'month']\n",
    "    lag_feats = [f'lag_{i}' for i in range(1, n_lags+1)]\n",
    "    numeric_features = datetime_feats + lag_feats\n",
    "    categorical_features = ['location_id']\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "    return Pipeline([('features', feature_pipeline), ('preprocess', preprocessor)])\n",
    "\n",
    "# --- Main Script ---\n",
    "if __name__ == '__main__':\n",
    "    import os\n",
    "\n",
    "    # Load the cleaned Parquet file\n",
    "    input_path = '/Users/kaushalshivaprakash/Desktop/project3/data/processed/cleaned_citibike/citibike_2023_top3.parquet'\n",
    "    df = pd.read_parquet(input_path)\n",
    "\n",
    "    # Rename and parse\n",
    "    df = df.rename(columns={'started_at':'datetime', 'start_station_id':'location_id'})\n",
    "    df['datetime'] = pd.to_datetime(df['datetime']).dt.floor('H')\n",
    "\n",
    "    # Aggregate to hourly trip counts\n",
    "    df_agg = (\n",
    "        df.groupby(['location_id', 'datetime']).size()\n",
    "          .reset_index(name='trip_count')\n",
    "    )\n",
    "\n",
    "    # Top 3 stations\n",
    "    top_locs = df_agg['location_id'].value_counts().nlargest(3).index.tolist()\n",
    "\n",
    "    # Train/test split: last 30 days as test\n",
    "    max_dt = df_agg['datetime'].max()\n",
    "    cutoff = max_dt - pd.Timedelta(days=30)\n",
    "    train = df_agg[df_agg['datetime'] < cutoff]\n",
    "    test  = df_agg[df_agg['datetime'] >= cutoff]\n",
    "\n",
    "    if train.empty or test.empty:\n",
    "        raise ValueError(\n",
    "            f\"Train or test split is empty! Train size: {len(train)}, Test size: {len(test)}\"\n",
    "        )\n",
    "\n",
    "    # Build, fit, and transform\n",
    "    pipeline = build_feature_pipeline(top_locs, n_lags=28)\n",
    "    X_train = pipeline.fit_transform(train)\n",
    "    X_test  = pipeline.transform(test)\n",
    "\n",
    "    # Ensure models directory exists\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "\n",
    "    # Save pipeline\n",
    "    model_path = 'models/feature_pipeline.pkl'\n",
    "    joblib.dump(pipeline, model_path)\n",
    "\n",
    "    # Confirmation messages\n",
    "    print(f'Pipeline built. Train samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}')\n",
    "    print(f'Feature engineering pipeline successfully saved to {model_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e92beb-7eb9-4801-a370-9294feaf9e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
